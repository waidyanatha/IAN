{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grab_text_classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waidyanatha/IAN/blob/master/grab_text_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "H4kgW_d-LXmJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prerequisit installations\n",
        "Run these installations at the onset of of each session"
      ]
    },
    {
      "metadata": {
        "id": "lOwXZDPklr8X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import necessary and sufficient Py files\n",
        "Shapely"
      ]
    },
    {
      "metadata": {
        "id": "yqIbRjZeEgdk",
        "colab_type": "code",
        "outputId": "c9846ff0-c258-4b5c-e30d-1dbd538685ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "!pip install -q matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "!pip install shapely"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package libfluidsynth1:amd64.\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 110845 files and directories currently installed.)\n",
            "Preparing to unpack .../libfluidsynth1_1.1.9-1_amd64.deb ...\n",
            "Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Collecting shapely\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/d1/b8e1b089a8ddd6df74be583d70373eac55c725c6197c115efbd3c3e1509f/Shapely-1.6.4.post2-cp27-cp27mu-manylinux1_x86_64.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 6.1MB/s \n",
            "\u001b[?25hInstalling collected packages: shapely\n",
            "Successfully installed shapely-1.6.4.post2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u-3gxJRiu7Kp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading local files\n",
        "* Clone from github [IAN project](https://github.com/waidyanatha/IAN) \n",
        "* Import: main.py, config.py, log.py, datafilter.py, classify.py, & plot.py\n",
        "* Possible methods for [local .py file import](https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab)\n"
      ]
    },
    {
      "metadata": {
        "id": "49SRF6p5rHXc",
        "colab_type": "code",
        "outputId": "faf17619-8849-4ef1-c5d8-dff4bea9b455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/waidyanatha/IAN.git\n",
        "%load IAN/main.py\n",
        "%load IAN/plot.py\n",
        "%load IAN/config.py\n",
        "%load IAN/log.py\n",
        "%load IAN/classify.py\n",
        "%load IAN/datafilter.py\n",
        "import shapely"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IAN'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/52)   \u001b[K\rremote: Counting objects:   3% (2/52)   \u001b[K\rremote: Counting objects:   5% (3/52)   \u001b[K\rremote: Counting objects:   7% (4/52)   \u001b[K\rremote: Counting objects:   9% (5/52)   \u001b[K\rremote: Counting objects:  11% (6/52)   \u001b[K\rremote: Counting objects:  13% (7/52)   \u001b[K\rremote: Counting objects:  15% (8/52)   \u001b[K\rremote: Counting objects:  17% (9/52)   \u001b[K\rremote: Counting objects:  19% (10/52)   \u001b[K\rremote: Counting objects:  21% (11/52)   \u001b[K\rremote: Counting objects:  23% (12/52)   \u001b[K\rremote: Counting objects:  25% (13/52)   \u001b[K\rremote: Counting objects:  26% (14/52)   \u001b[K\rremote: Counting objects:  28% (15/52)   \u001b[K\rremote: Counting objects:  30% (16/52)   \u001b[K\rremote: Counting objects:  32% (17/52)   \u001b[K\rremote: Counting objects:  34% (18/52)   \u001b[K\rremote: Counting objects:  36% (19/52)   \u001b[K\rremote: Counting objects:  38% (20/52)   \u001b[K\rremote: Counting objects:  40% (21/52)   \u001b[K\rremote: Counting objects:  42% (22/52)   \u001b[K\rremote: Counting objects:  44% (23/52)   \u001b[K\rremote: Counting objects:  46% (24/52)   \u001b[K\rremote: Counting objects:  48% (25/52)   \u001b[K\rremote: Counting objects:  50% (26/52)   \u001b[K\rremote: Counting objects:  51% (27/52)   \u001b[K\rremote: Counting objects:  53% (28/52)   \u001b[K\rremote: Counting objects:  55% (29/52)   \u001b[K\rremote: Counting objects:  57% (30/52)   \u001b[K\rremote: Counting objects:  59% (31/52)   \u001b[K\rremote: Counting objects:  61% (32/52)   \u001b[K\rremote: Counting objects:  63% (33/52)   \u001b[K\rremote: Counting objects:  65% (34/52)   \u001b[K\rremote: Counting objects:  67% (35/52)   \u001b[K\rremote: Counting objects:  69% (36/52)   \u001b[K\rremote: Counting objects:  71% (37/52)   \u001b[K\rremote: Counting objects:  73% (38/52)   \u001b[K\rremote: Counting objects:  75% (39/52)   \u001b[K\rremote: Counting objects:  76% (40/52)   \u001b[K\rremote: Counting objects:  78% (41/52)   \u001b[K\rremote: Counting objects:  80% (42/52)   \u001b[K\rremote: Counting objects:  82% (43/52)   \u001b[K\rremote: Counting objects:  84% (44/52)   \u001b[K\rremote: Counting objects:  86% (45/52)   \u001b[K\rremote: Counting objects:  88% (46/52)   \u001b[K\rremote: Counting objects:  90% (47/52)   \u001b[K\rremote: Counting objects:  92% (48/52)   \u001b[K\rremote: Counting objects:  94% (49/52)   \u001b[K\rremote: Counting objects:  96% (50/52)   \u001b[K\rremote: Counting objects:  98% (51/52)   \u001b[K\rremote: Counting objects: 100% (52/52)   \u001b[K\rremote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/37)   \u001b[K\rremote: Compressing objects:   5% (2/37)   \u001b[K\rremote: Compressing objects:   8% (3/37)   \u001b[K\rremote: Compressing objects:  10% (4/37)   \u001b[K\rremote: Compressing objects:  13% (5/37)   \u001b[K\rremote: Compressing objects:  16% (6/37)   \u001b[K\rremote: Compressing objects:  18% (7/37)   \u001b[K\rremote: Compressing objects:  21% (8/37)   \u001b[K\rremote: Compressing objects:  24% (9/37)   \u001b[K\rremote: Compressing objects:  27% (10/37)   \u001b[K\rremote: Compressing objects:  29% (11/37)   \u001b[K\rremote: Compressing objects:  32% (12/37)   \u001b[K\rremote: Compressing objects:  35% (13/37)   \u001b[K\rremote: Compressing objects:  37% (14/37)   \u001b[K\rremote: Compressing objects:  40% (15/37)   \u001b[K\rremote: Compressing objects:  43% (16/37)   \u001b[K\rremote: Compressing objects:  45% (17/37)   \u001b[K\rremote: Compressing objects:  48% (18/37)   \u001b[K\rremote: Compressing objects:  51% (19/37)   \u001b[K\rremote: Compressing objects:  54% (20/37)   \u001b[K\rremote: Compressing objects:  56% (21/37)   \u001b[K\rremote: Compressing objects:  59% (22/37)   \u001b[K\rremote: Compressing objects:  62% (23/37)   \u001b[K\rremote: Compressing objects:  64% (24/37)   \u001b[K\rremote: Compressing objects:  67% (25/37)   \u001b[K\rremote: Compressing objects:  70% (26/37)   \u001b[K\rremote: Compressing objects:  72% (27/37)   \u001b[K\rremote: Compressing objects:  75% (28/37)   \u001b[K\rremote: Compressing objects:  78% (29/37)   \u001b[K\rremote: Compressing objects:  81% (30/37)   \u001b[K\rremote: Compressing objects:  83% (31/37)   \u001b[K\rremote: Compressing objects:  86% (32/37)   \u001b[K\rremote: Compressing objects:  89% (33/37)   \u001b[K\rremote: Compressing objects:  91% (34/37)   \u001b[K\rremote: Compressing objects:  94% (35/37)   \u001b[K\rremote: Compressing objects:  97% (36/37)   \u001b[K\rremote: Compressing objects: 100% (37/37)   \u001b[K\rremote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 52 (delta 30), reused 37 (delta 15), pack-reused 0\u001b[K\n",
            "Unpacking objects:   1% (1/52)   \rUnpacking objects:   3% (2/52)   \rUnpacking objects:   5% (3/52)   \rUnpacking objects:   7% (4/52)   \rUnpacking objects:   9% (5/52)   \rUnpacking objects:  11% (6/52)   \rUnpacking objects:  13% (7/52)   \rUnpacking objects:  15% (8/52)   \rUnpacking objects:  17% (9/52)   \rUnpacking objects:  19% (10/52)   \rUnpacking objects:  21% (11/52)   \rUnpacking objects:  23% (12/52)   \rUnpacking objects:  25% (13/52)   \rUnpacking objects:  26% (14/52)   \rUnpacking objects:  28% (15/52)   \rUnpacking objects:  30% (16/52)   \rUnpacking objects:  32% (17/52)   \rUnpacking objects:  34% (18/52)   \rUnpacking objects:  36% (19/52)   \rUnpacking objects:  38% (20/52)   \rUnpacking objects:  40% (21/52)   \rUnpacking objects:  42% (22/52)   \rUnpacking objects:  44% (23/52)   \rUnpacking objects:  46% (24/52)   \rUnpacking objects:  48% (25/52)   \rUnpacking objects:  50% (26/52)   \rUnpacking objects:  51% (27/52)   \rUnpacking objects:  53% (28/52)   \rUnpacking objects:  55% (29/52)   \rUnpacking objects:  57% (30/52)   \rUnpacking objects:  59% (31/52)   \rUnpacking objects:  61% (32/52)   \rUnpacking objects:  63% (33/52)   \rUnpacking objects:  65% (34/52)   \rUnpacking objects:  67% (35/52)   \rUnpacking objects:  69% (36/52)   \rUnpacking objects:  71% (37/52)   \rUnpacking objects:  73% (38/52)   \rUnpacking objects:  75% (39/52)   \rUnpacking objects:  76% (40/52)   \rUnpacking objects:  78% (41/52)   \rUnpacking objects:  80% (42/52)   \rUnpacking objects:  82% (43/52)   \rUnpacking objects:  84% (44/52)   \rUnpacking objects:  86% (45/52)   \rUnpacking objects:  88% (46/52)   \rUnpacking objects:  90% (47/52)   \rUnpacking objects:  92% (48/52)   \rUnpacking objects:  94% (49/52)   \rUnpacking objects:  96% (50/52)   \rUnpacking objects:  98% (51/52)   \rUnpacking objects: 100% (52/52)   \rUnpacking objects: 100% (52/52), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "01yTZgjfdP2P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "980856c9-439f-4235-cbef-39ead43cb92e"
      },
      "cell_type": "code",
      "source": [
        "!ls -al IAN"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 44\n",
            "drwxr-xr-x 3 root root  4096 Jan  8 07:36 .\n",
            "drwxr-xr-x 1 root root  4096 Jan  8 07:36 ..\n",
            "-rw-r--r-- 1 root root  2181 Jan  8 07:36 classify.py\n",
            "-rw-r--r-- 1 root root  1392 Jan  8 07:36 config.py\n",
            "-rw-r--r-- 1 root root  2720 Jan  8 07:36 datafilter.py\n",
            "drwxr-xr-x 8 root root  4096 Jan  8 07:36 .git\n",
            "-rw-r--r-- 1 root root   816 Jan  8 07:36 log.py\n",
            "-rw-r--r-- 1 root root 10239 Jan  8 07:36 main.py\n",
            "-rwxr-xr-x 1 root root  3107 Jan  8 07:36 plot.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DQkNeFwO8ywp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Upload the message data"
      ]
    },
    {
      "metadata": {
        "id": "H90qE_VQ9QCS",
        "colab_type": "code",
        "outputId": "72993eeb-7e56-428c-8749-5ef90e9152b8",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "from google.colab import files\n",
        "src = list(files.upload().values())[0]\n",
        "open('100_cleaned_alerts.json','wb').write(src)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0809faa5-b81a-453b-8de6-398ad58c6a6b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0809faa5-b81a-453b-8de6-398ad58c6a6b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 100_cleaned_alerts.json to 100_cleaned_alerts.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BghTkzK4IIlF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "# alert data from GRAB DB (see config.py for file name)\n",
        "file_id = '1_KcdQSeR8HwH4qhCIzx-qqLL0y4bSy6vjyObha2V5v0'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "#print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9lOS736OK0Hh",
        "colab_type": "code",
        "outputId": "c653455a-dd12-41ed-b6dd-66889913f06d",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "from google.colab import files\n",
        "src = list(files.upload().values())[0]\n",
        "open('config.py','wb').write(src)\n",
        "import config\n",
        "open('log.py','wb').write(src)\n",
        "import log\n",
        "open('datafilter.py','wb').write(src)\n",
        "import datafilter\n",
        "open('plot.py','wb').write(src)\n",
        "import plot\n",
        "open('classify.py','wb').write(src)\n",
        "import classify\n",
        "open('main.py','wb').write(src)\n",
        "import main\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7d59ef6c-e276-473a-9eeb-041d8977ece5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7d59ef6c-e276-473a-9eeb-041d8977ece5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9b7c0aaffcba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'config.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "FzHQNLtX9Bxs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run the classification\n",
        "* [import .py instructions](https://medium.com/lean-in-women-in-tech-india/google-colab-the-beginners-guide-5ad3b417dfa) om Medium with "
      ]
    },
    {
      "metadata": {
        "id": "ZQaqRy1VgH0o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mv 100_test_alerts.csv ./data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-0f_0u37WI3C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4083
        },
        "outputId": "a518cf67-034e-430b-9ff3-6321b8e13bf7"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('IAN')\n",
        "!ls -al\n",
        "!python IAN/main.py"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 44\n",
            "drwxr-xr-x 1 root root 4096 Jan  8 12:47 .\n",
            "drwxr-xr-x 1 root root 4096 Jan  8 11:34 ..\n",
            "-rw-r--r-- 1 root root 2518 Jan  8 12:39 adc.json\n",
            "drwxr-xr-x 1 root root 4096 Jan  8 12:39 .config\n",
            "drwxr-xr-x 2 root root 4096 Jan  8 12:47 data\n",
            "-rw-r--r-- 1 root root 1753 Jan  8 12:39 grab.log\n",
            "drwxr-xr-x 3 root root 4096 Jan  8 12:39 IAN\n",
            "drwxr-xr-x 2 root root 4096 Jan  8 12:39 plots\n",
            "drwxr-xr-x 1 root root 4096 Jan  3 17:15 sample_data\n",
            "/content/IAN/plot.py:20: UserWarning: \n",
            "This call to matplotlib.use() has no effect because the backend has already\n",
            "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
            "or matplotlib.backends is imported for the first time.\n",
            "\n",
            "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
            "  File \"IAN/main.py\", line 14, in <module>\n",
            "    import classify as clfy\n",
            "  File \"/content/IAN/classify.py\", line 7, in <module>\n",
            "    import matplotlib.pyplot as plt\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\", line 72, in <module>\n",
            "    from matplotlib.backends import pylab_setup\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
            "    line for line in traceback.format_stack()\n",
            "\n",
            "\n",
            "  matplotlib.use('Agg')\n",
            "2019-01-08 12:48:02.664703: begin message text classification with TF and NLP \n",
            "2019-01-08 12:48:02.778800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-01-08 12:48:02.779456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-01-08 12:48:02.779499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
            "2019-01-08 12:48:03.204702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-01-08 12:48:03.204780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
            "2019-01-08 12:48:03.204806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
            "2019-01-08 12:48:03.205107: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-01-08 12:48:03.205192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 7834507788307140771\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 6875064637685104335\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 2410386788689176032\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11281553818\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 636219254313269767\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n",
            "2019-01-08 12:48:20.722243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
            "2019-01-08 12:48:20.722336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-01-08 12:48:20.722381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
            "2019-01-08 12:48:20.722408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
            "2019-01-08 12:48:20.722706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2019-01-08 12:48:40.918078: E tensorflow/core/common_runtime/executor.cc:623] Executor failed to create kernel. Not found: No registered 'Size' OpKernel for GPU devices compatible with node {{node module_1_apply_default/compound_bigrams/boolean_mask/Prod}} = Size[T=DT_STRING, out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](module_1_apply_default/compound_bigrams/concat_2/_155, ^module_1_apply_default/compound_bigrams/boolean_mask/Shape_1/_159)\n",
            "\t (OpKernel was found, but attributes didn't match)\n",
            "\t.  Registered:  device='XLA_CPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF, DT_UINT32, DT_UINT64]\n",
            "  device='XLA_GPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]\n",
            "  device='XLA_CPU'; out_type in [DT_INT64]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\n",
            "  device='XLA_CPU'; out_type in [DT_INT32]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\n",
            "  device='XLA_GPU'; out_type in [DT_INT64]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL, DT_BFLOAT16]\n",
            "  device='XLA_GPU'; out_type in [DT_INT32]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL, DT_BFLOAT16]\n",
            "  device='CPU'; out_type in [DT_INT64]\n",
            "  device='CPU'; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT32]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT32]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_VARIANT]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_VARIANT]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_BOOL]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_BOOL]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_COMPLEX128]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_COMPLEX128]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_COMPLEX64]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_COMPLEX64]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT8]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT8]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_UINT8]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_UINT8]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_UINT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_UINT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT64]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT64]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_DOUBLE]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_DOUBLE]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_FLOAT]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_FLOAT]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_BFLOAT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_BFLOAT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_HALF]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_HALF]; out_type in [DT_INT32]\n",
            "\n",
            "\t [[{{node module_1_apply_default/compound_bigrams/boolean_mask/Prod}} = Size[T=DT_STRING, out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](module_1_apply_default/compound_bigrams/concat_2/_155, ^module_1_apply_default/compound_bigrams/boolean_mask/Shape_1/_159)]]\n",
            "Traceback (most recent call last):\n",
            "  File \"IAN/main.py\", line 109, in <module>\n",
            "    error_count += clfy.sentence_encoder(messages);\n",
            "  File \"/content/IAN/classify.py\", line 29, in sentence_encoder\n",
            "    message_embeddings = session.run(embed(messages))\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Size' OpKernel for GPU devices compatible with node node module_1_apply_default/compound_bigrams/boolean_mask/Prod (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_hub/native_module.py:514)  = Size[T=DT_STRING, out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](module_1_apply_default/compound_bigrams/concat_2/_155, ^module_1_apply_default/compound_bigrams/boolean_mask/Shape_1/_159)\n",
            "\t (OpKernel was found, but attributes didn't match)\n",
            "\t.  Registered:  device='XLA_CPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF, DT_UINT32, DT_UINT64]\n",
            "  device='XLA_GPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]\n",
            "  device='XLA_CPU'; out_type in [DT_INT64]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\n",
            "  device='XLA_CPU'; out_type in [DT_INT32]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\n",
            "  device='XLA_GPU'; out_type in [DT_INT64]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL, DT_BFLOAT16]\n",
            "  device='XLA_GPU'; out_type in [DT_INT32]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL, DT_BFLOAT16]\n",
            "  device='CPU'; out_type in [DT_INT64]\n",
            "  device='CPU'; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT32]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT32]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_VARIANT]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_VARIANT]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_BOOL]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_BOOL]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_COMPLEX128]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_COMPLEX128]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_COMPLEX64]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_COMPLEX64]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT8]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT8]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_UINT8]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_UINT8]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_UINT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_UINT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT64]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT64]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_DOUBLE]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_DOUBLE]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_FLOAT]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_FLOAT]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_BFLOAT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_BFLOAT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_HALF]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_HALF]; out_type in [DT_INT32]\n",
            "\n",
            "\t [[node module_1_apply_default/compound_bigrams/boolean_mask/Prod (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_hub/native_module.py:514)  = Size[T=DT_STRING, out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](module_1_apply_default/compound_bigrams/concat_2/_155, ^module_1_apply_default/compound_bigrams/boolean_mask/Shape_1/_159)]]\n",
            "\n",
            "Caused by op u'module_1_apply_default/compound_bigrams/boolean_mask/Prod', defined at:\n",
            "  File \"IAN/main.py\", line 109, in <module>\n",
            "    error_count += clfy.sentence_encoder(messages);\n",
            "  File \"/content/IAN/classify.py\", line 29, in sentence_encoder\n",
            "    message_embeddings = session.run(embed(messages))\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_hub/module.py\", line 247, in __call__\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_hub/native_module.py\", line 514, in create_apply_graph\n",
            "    import_scope=relative_scope_name)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1674, in import_meta_graph\n",
            "    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1696, in _import_meta_graph_with_return_elements\n",
            "    **kwargs))\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\n",
            "    return_elements=return_elements)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 442, in import_graph_def\n",
            "    _ProcessNewOps(graph)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 234, in _ProcessNewOps\n",
            "    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3440, in _add_new_tf_operations\n",
            "    for c_op in c_api_util.new_tf_operations(self)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3299, in _create_op_from_tf_operation\n",
            "    ret = Operation(c_op, self)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "NotFoundError (see above for traceback): No registered 'Size' OpKernel for GPU devices compatible with node node module_1_apply_default/compound_bigrams/boolean_mask/Prod (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_hub/native_module.py:514)  = Size[T=DT_STRING, out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](module_1_apply_default/compound_bigrams/concat_2/_155, ^module_1_apply_default/compound_bigrams/boolean_mask/Shape_1/_159)\n",
            "\t (OpKernel was found, but attributes didn't match)\n",
            "\t.  Registered:  device='XLA_CPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF, DT_UINT32, DT_UINT64]\n",
            "  device='XLA_GPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]\n",
            "  device='XLA_CPU'; out_type in [DT_INT64]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\n",
            "  device='XLA_CPU'; out_type in [DT_INT32]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\n",
            "  device='XLA_GPU'; out_type in [DT_INT64]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL, DT_BFLOAT16]\n",
            "  device='XLA_GPU'; out_type in [DT_INT32]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL, DT_BFLOAT16]\n",
            "  device='CPU'; out_type in [DT_INT64]\n",
            "  device='CPU'; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT32]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT32]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_VARIANT]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_VARIANT]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_BOOL]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_BOOL]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_COMPLEX128]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_COMPLEX128]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_COMPLEX64]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_COMPLEX64]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT8]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT8]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_UINT8]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_UINT8]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_UINT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_UINT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT64]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT64]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_DOUBLE]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_DOUBLE]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_FLOAT]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_FLOAT]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_BFLOAT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_BFLOAT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_HALF]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_HALF]; out_type in [DT_INT32]\n",
            "\n",
            "\t [[node module_1_apply_default/compound_bigrams/boolean_mask/Prod (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_hub/native_module.py:514)  = Size[T=DT_STRING, out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](module_1_apply_default/compound_bigrams/concat_2/_155, ^module_1_apply_default/compound_bigrams/boolean_mask/Shape_1/_159)]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}