{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grab_text_classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waidyanatha/IAN/blob/master/grab_text_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "H4kgW_d-LXmJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prerequisit installations\n",
        "Run these installations at the onset of of each session"
      ]
    },
    {
      "metadata": {
        "id": "lOwXZDPklr8X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import necessary and sufficient Py files\n",
        "Shapely"
      ]
    },
    {
      "metadata": {
        "id": "yqIbRjZeEgdk",
        "colab_type": "code",
        "outputId": "a49ac3ed-c717-4966-e545-2e5d329da6b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "!pip install -q matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "!pip install shapely\n",
        "!pip install seaborn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: shapely in /usr/local/lib/python2.7/dist-packages (1.6.4.post2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python2.7/dist-packages (0.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u-3gxJRiu7Kp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading local files\n",
        "* Clone from github [IAN project](https://github.com/waidyanatha/IAN) \n",
        "* Import: main.py, config.py, log.py, datafilter.py, classify.py, & plot.py\n",
        "* Possible methods for [local .py file import](https://stackoverflow.com/questions/48905127/importing-py-files-in-google-colab)\n"
      ]
    },
    {
      "metadata": {
        "id": "49SRF6p5rHXc",
        "colab_type": "code",
        "outputId": "9e058beb-14da-4e3f-8db9-c99eb87cd918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/waidyanatha/IAN.git\n",
        "%load IAN/main.py\n",
        "%load IAN/plot.py\n",
        "%load IAN/config.py\n",
        "%load IAN/log.py\n",
        "%load IAN/classify.py\n",
        "%load IAN/datafilter.py\n",
        "import shapely"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'IAN' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DQkNeFwO8ywp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Upload the message data\n",
        "* Connect to your google drive to upload the data files from there\n",
        "* Other options (not implemented) involve loading from local storage"
      ]
    },
    {
      "metadata": {
        "id": "BghTkzK4IIlF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "# alert data from GRAB DB (see config.py for file name)\n",
        "file_id = '1_KcdQSeR8HwH4qhCIzx-qqLL0y4bSy6vjyObha2V5v0'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "#print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FzHQNLtX9Bxs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run the classification\n",
        "* [import .py instructions](https://medium.com/lean-in-women-in-tech-india/google-colab-the-beginners-guide-5ad3b417dfa) om Medium with "
      ]
    },
    {
      "metadata": {
        "id": "TO-K9GCHjlNY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Copy the data file into ./data"
      ]
    },
    {
      "metadata": {
        "id": "ZQaqRy1VgH0o",
        "colab_type": "code",
        "outputId": "02263c55-3570-490d-c067-ab8ea949f220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!cp 100_test_alerts.csv ./data/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '100_test_alerts.csv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "citOJOwWk_VS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Run main.py"
      ]
    },
    {
      "metadata": {
        "id": "-0f_0u37WI3C",
        "colab_type": "code",
        "outputId": "67e43376-2704-4b0e-d7ac-fc6bcd0f5596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4100
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('IAN')\n",
        "!ls -al\n",
        "!python IAN/main.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 48\n",
            "drwxr-xr-x 1 root root 4096 Jan  8 13:45 .\n",
            "drwxr-xr-x 1 root root 4096 Jan  8 11:34 ..\n",
            "-rw-r--r-- 1 root root 2518 Jan  8 12:39 adc.json\n",
            "drwxr-xr-x 1 root root 4096 Jan  8 12:39 .config\n",
            "drwxr-xr-x 2 root root 4096 Jan  8 12:48 data\n",
            "-rw-r--r-- 1 root root 3644 Jan  8 14:45 grab.log\n",
            "drwxr-xr-x 3 root root 4096 Jan  8 12:39 IAN\n",
            "drwxr-xr-x 2 root root 4096 Jan  8 13:45 .ipynb_checkpoints\n",
            "drwxr-xr-x 2 root root 4096 Jan  8 12:39 plots\n",
            "drwxr-xr-x 1 root root 4096 Jan  3 17:15 sample_data\n",
            "/content/IAN/plot.py:20: UserWarning: \n",
            "This call to matplotlib.use() has no effect because the backend has already\n",
            "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
            "or matplotlib.backends is imported for the first time.\n",
            "\n",
            "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
            "  File \"IAN/main.py\", line 14, in <module>\n",
            "    import classify as clfy\n",
            "  File \"/content/IAN/classify.py\", line 7, in <module>\n",
            "    import matplotlib.pyplot as plt\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\", line 72, in <module>\n",
            "    from matplotlib.backends import pylab_setup\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
            "    line for line in traceback.format_stack()\n",
            "\n",
            "\n",
            "  matplotlib.use('Agg')\n",
            "2019-01-08 14:46:03.000814: begin message text classification with TF and NLP \n",
            "2019-01-08 14:46:03.116419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-01-08 14:46:03.116879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-01-08 14:46:03.116921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
            "2019-01-08 14:46:03.539356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-01-08 14:46:03.539435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
            "2019-01-08 14:46:03.539460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
            "2019-01-08 14:46:03.539817: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-01-08 14:46:03.539887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 6100305066278824216\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 8286704609540134318\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 9767797879349933046\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11281553818\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 13691141043744106080\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n",
            "2019-01-08 14:46:07.054986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
            "2019-01-08 14:46:07.055081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-01-08 14:46:07.055112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
            "2019-01-08 14:46:07.055139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
            "2019-01-08 14:46:07.055452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "2019-01-08 14:46:27.625473: E tensorflow/core/common_runtime/executor.cc:623] Executor failed to create kernel. Not found: No registered 'Size' OpKernel for GPU devices compatible with node {{node module_1_apply_default/compound_bigrams/boolean_mask/Prod}} = Size[T=DT_STRING, out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](module_1_apply_default/compound_bigrams/concat_2/_155, ^module_1_apply_default/compound_bigrams/boolean_mask/Shape_1/_159)\n",
            "\t (OpKernel was found, but attributes didn't match)\n",
            "\t.  Registered:  device='XLA_CPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF, DT_UINT32, DT_UINT64]\n",
            "  device='XLA_GPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]\n",
            "  device='XLA_CPU'; out_type in [DT_INT64]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\n",
            "  device='XLA_CPU'; out_type in [DT_INT32]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\n",
            "  device='XLA_GPU'; out_type in [DT_INT64]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL, DT_BFLOAT16]\n",
            "  device='XLA_GPU'; out_type in [DT_INT32]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL, DT_BFLOAT16]\n",
            "  device='CPU'; out_type in [DT_INT64]\n",
            "  device='CPU'; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT32]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT32]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_VARIANT]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_VARIANT]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_BOOL]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_BOOL]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_COMPLEX128]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_COMPLEX128]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_COMPLEX64]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_COMPLEX64]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT8]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT8]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_UINT8]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_UINT8]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_UINT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_UINT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT64]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT64]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_DOUBLE]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_DOUBLE]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_FLOAT]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_FLOAT]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_BFLOAT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_BFLOAT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_HALF]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_HALF]; out_type in [DT_INT32]\n",
            "\n",
            "\t [[{{node module_1_apply_default/compound_bigrams/boolean_mask/Prod}} = Size[T=DT_STRING, out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](module_1_apply_default/compound_bigrams/concat_2/_155, ^module_1_apply_default/compound_bigrams/boolean_mask/Shape_1/_159)]]\n",
            "Traceback (most recent call last):\n",
            "  File \"IAN/main.py\", line 109, in <module>\n",
            "    error_count += clfy.sentence_encoder(messages);\n",
            "  File \"/content/IAN/classify.py\", line 29, in sentence_encoder\n",
            "    message_embeddings = session.run(embed(messages))\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Size' OpKernel for GPU devices compatible with node node module_1_apply_default/compound_bigrams/boolean_mask/Prod (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_hub/native_module.py:514)  = Size[T=DT_STRING, out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](module_1_apply_default/compound_bigrams/concat_2/_155, ^module_1_apply_default/compound_bigrams/boolean_mask/Shape_1/_159)\n",
            "\t (OpKernel was found, but attributes didn't match)\n",
            "\t.  Registered:  device='XLA_CPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF, DT_UINT32, DT_UINT64]\n",
            "  device='XLA_GPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]\n",
            "  device='XLA_CPU'; out_type in [DT_INT64]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\n",
            "  device='XLA_CPU'; out_type in [DT_INT32]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\n",
            "  device='XLA_GPU'; out_type in [DT_INT64]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL, DT_BFLOAT16]\n",
            "  device='XLA_GPU'; out_type in [DT_INT32]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL, DT_BFLOAT16]\n",
            "  device='CPU'; out_type in [DT_INT64]\n",
            "  device='CPU'; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT32]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT32]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_VARIANT]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_VARIANT]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_BOOL]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_BOOL]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_COMPLEX128]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_COMPLEX128]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_COMPLEX64]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_COMPLEX64]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT8]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT8]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_UINT8]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_UINT8]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_UINT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_UINT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT64]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT64]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_DOUBLE]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_DOUBLE]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_FLOAT]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_FLOAT]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_BFLOAT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_BFLOAT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_HALF]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_HALF]; out_type in [DT_INT32]\n",
            "\n",
            "\t [[node module_1_apply_default/compound_bigrams/boolean_mask/Prod (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_hub/native_module.py:514)  = Size[T=DT_STRING, out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](module_1_apply_default/compound_bigrams/concat_2/_155, ^module_1_apply_default/compound_bigrams/boolean_mask/Shape_1/_159)]]\n",
            "\n",
            "Caused by op u'module_1_apply_default/compound_bigrams/boolean_mask/Prod', defined at:\n",
            "  File \"IAN/main.py\", line 109, in <module>\n",
            "    error_count += clfy.sentence_encoder(messages);\n",
            "  File \"/content/IAN/classify.py\", line 29, in sentence_encoder\n",
            "    message_embeddings = session.run(embed(messages))\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_hub/module.py\", line 247, in __call__\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_hub/native_module.py\", line 514, in create_apply_graph\n",
            "    import_scope=relative_scope_name)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1674, in import_meta_graph\n",
            "    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1696, in _import_meta_graph_with_return_elements\n",
            "    **kwargs))\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\n",
            "    return_elements=return_elements)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 442, in import_graph_def\n",
            "    _ProcessNewOps(graph)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 234, in _ProcessNewOps\n",
            "    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3440, in _add_new_tf_operations\n",
            "    for c_op in c_api_util.new_tf_operations(self)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3299, in _create_op_from_tf_operation\n",
            "    ret = Operation(c_op, self)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "NotFoundError (see above for traceback): No registered 'Size' OpKernel for GPU devices compatible with node node module_1_apply_default/compound_bigrams/boolean_mask/Prod (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_hub/native_module.py:514)  = Size[T=DT_STRING, out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](module_1_apply_default/compound_bigrams/concat_2/_155, ^module_1_apply_default/compound_bigrams/boolean_mask/Shape_1/_159)\n",
            "\t (OpKernel was found, but attributes didn't match)\n",
            "\t.  Registered:  device='XLA_CPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_HALF, DT_UINT32, DT_UINT64]\n",
            "  device='XLA_GPU_JIT'; out_type in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT8, ..., DT_QINT32, DT_BFLOAT16, DT_HALF, DT_UINT32, DT_UINT64]\n",
            "  device='XLA_CPU'; out_type in [DT_INT64]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\n",
            "  device='XLA_CPU'; out_type in [DT_INT32]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL]\n",
            "  device='XLA_GPU'; out_type in [DT_INT64]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL, DT_BFLOAT16]\n",
            "  device='XLA_GPU'; out_type in [DT_INT32]; T in [DT_UINT8, DT_QUINT8, DT_INT8, DT_QINT8, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BOOL, DT_BFLOAT16]\n",
            "  device='CPU'; out_type in [DT_INT64]\n",
            "  device='CPU'; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT32]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT32]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_VARIANT]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_VARIANT]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_BOOL]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_BOOL]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_COMPLEX128]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_COMPLEX128]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_COMPLEX64]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_COMPLEX64]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT8]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT8]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_UINT8]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_UINT8]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_UINT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_UINT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_INT64]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_INT64]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_DOUBLE]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_DOUBLE]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_FLOAT]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_FLOAT]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_BFLOAT16]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_BFLOAT16]; out_type in [DT_INT32]\n",
            "  device='GPU'; T in [DT_HALF]; out_type in [DT_INT64]\n",
            "  device='GPU'; T in [DT_HALF]; out_type in [DT_INT32]\n",
            "\n",
            "\t [[node module_1_apply_default/compound_bigrams/boolean_mask/Prod (defined at /usr/local/lib/python2.7/dist-packages/tensorflow_hub/native_module.py:514)  = Size[T=DT_STRING, out_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](module_1_apply_default/compound_bigrams/concat_2/_155, ^module_1_apply_default/compound_bigrams/boolean_mask/Shape_1/_159)]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}